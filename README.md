# ğŸŒŸ PPG Blood Glucose Diabetes Classification

Welcome to the **PPG Blood Glucose Diabetes Classification** project! ğŸ‰  
This repository provides a pipeline for estimating blood glucose levels using **Photoplethysmography (PPG)** signals.  
By combining advanced signal processing, machine learning, and visualizations, we enable **non-invasive diabetes screening** for modern preventive healthcare. ğŸ©ºğŸ’¡

---

## ğŸ“‹ Project Overview

This project uses **PPG**â€”an optical technique for capturing blood volume changesâ€”to **predict blood glucose levels non-invasively**.

The modular framework integrates signal engineering and machine learning, making it suitable for experimentation and real-world use.

### âœ¨ Key Features

- ğŸ“Š **Data Processing**: Preprocess raw PPG segments and extract physiological features  
- ğŸ¤– **Machine Learning Models**: Random Forest, Gradient Boosting, SVM, LightGBM, Logistic Regression, and Ensemble Methods, Stacking and Voting Classifiers  
- ğŸ“ˆ **Visualizations**: ROC curves, confusion matrices, and feature importance plots  
- ğŸ§ª **Evaluation**: Subject-wise StratifiedGroupKFold cross-validation to prevent data leakage and ensure real-world applicability  

---

## ğŸ“‚ Project Structure

```
PPG_Blood_Glucose_JB_Implementation/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ ppg_bagging_tree_features.csv
â”‚   â”œâ”€â”€ ppg_specific_features.csv
â”‚   â”œâ”€â”€ processed_metadata.csv
â”‚   â”œâ”€â”€ PPG-BP.xlsx
â”‚   â””â”€â”€ 0_subject/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ random_forest.pkl
â”‚   â”œâ”€â”€ svm.pkl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ confusion_matrix_randomforest.png
â”‚   â”œâ”€â”€ roc_randomforest.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ excel_handling.py
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ train_models.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## ğŸš€ Getting Started

Letâ€™s get you up and running in no time!

### âœ… Prerequisites

- Python 3.8+ ğŸ  
- pip (Python package manager)  
- Git ğŸ“¦

---

### ğŸ›  Setup Instructions

#### 1. Clone the Repository

```bash
git clone https://github.com/Spidy104/PPG_DIABETES_CLASSIFICATION
cd PPG_DIABETES_CLASSIFICATION
```
#### 2. Set Up a Virtual Environment (Recommended)

```bash
python -m venv venv
```

#### 3. Activate the Virtual Environment

- **On macOS/Linux:**
    ```bash
    source venv/bin/activate
    ```
- **On Windows:**
    ```bash
    venv\Scripts\activate
    ```

#### 4. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Usage Guide

### ğŸ“ Step 1: Prepare the Data

Ensure the `datasets/` folder contains:

- `ppg_bagging_tree_features.csv` â€” Contains extracted features from PPG signals using bagging tree methods for model training.
- `ppg_specific_features.csv` â€” Includes domain-specific physiological features derived from PPG signals.
- `processed_metadata.csv` â€” Metadata for each sample, such as subject IDs, timestamps, and labels (e.g., glucose levels).
- `PPG-BP.xlsx` â€” Raw and reference data, including PPG signals and corresponding blood pressure/glucose measurements.
- `0_subject/` (Raw PPG signals by subject) â€” Directory with raw PPG signal files, organized per subject for preprocessing.
### âš™ï¸ Step 2: Process and Preprocess Data

#### 2.1 Process Excel Metadata

```bash
python src/excel_handling.py
```

#### 2.2 Preprocess Raw Data

```bash
python src/data_preprocessing.py
```
#### 2.3 Feature Extraction

```bash
python src/feature_extraction.py
```

### ğŸ‹ï¸ Step 3: Train the Models

Run the following command to train all machine learning models:

```bash
python src/train_models.py
```

This will generate model files in the `models/` directory:

```
models/
â”œâ”€â”€ random_forest.pkl
â”œâ”€â”€ svm.pkl
â”œâ”€â”€ gradient_boosting.pkl
â”œâ”€â”€ lightgbm.pkl
â”œâ”€â”€ logistic_regression.pkl
â”œâ”€â”€ stacking_classifier.pkl
â””â”€â”€ voting_classifier.pkl
```

### ğŸ“Š Step 4: Evaluate and Visualize Results

Evaluate the trained models and generate visualizations:

```bash
python src/second_model.ipynb
```

Results and plots will be saved in the `outputs/` directory:

```plaintext
outputs/
â”œâ”€â”€ gradient_boosting.jpg
â”œâ”€â”€ LightGBM.jpg
â”œâ”€â”€ Stacking_Classifier.jpg
â”œâ”€â”€ Voting_Classifier.jpg
â”œâ”€â”€ Model_performance.jpg
â”œâ”€â”€ ROC_curves.jpg
```

---

## âœ… Example Outputs

Hereâ€™s a sneak peek at the insights you'll get:

### ğŸ§¾ Classification Report

Below are the classification reports for each model (Model 2):

#### Random Forest

| Class      | Precision | Recall | F1-Score | Support |
|------------|-----------|--------|----------|---------|
| 0          | 0.830     | 0.990  | 0.900    | 181     |
| 1          | 0.000     | 0.000  | 0.000    | 38      |
| **Accuracy**   |           |        | 0.820    | 219     |
| **Macro Avg**  | 0.410     | 0.500  | 0.450    | 219     |
| **Weighted Avg** | 0.680   | 0.820  | 0.750    | 219     |

#### SVM

| Class      | Precision | Recall | F1-Score | Support |
|------------|-----------|--------|----------|---------|
| 0          | 0.830     | 1.000  | 0.910    | 181     |
| 1          | 0.000     | 0.000  | 0.000    | 38      |
| **Accuracy**   |           |        | 0.830    | 219     |
| **Macro Avg**  | 0.410     | 0.500  | 0.450    | 219     |
| **Weighted Avg** | 0.680   | 0.830  | 0.750    | 219     |

#### Gradient Boosting

| Class      | Precision | Recall | F1-Score | Support |
|------------|-----------|--------|----------|---------|
| 0          | 0.830     | 0.980  | 0.900    | 181     |
| 1          | 0.330     | 0.050  | 0.090    | 38      |
| **Accuracy**   |           |        | 0.820    | 219     |
| **Macro Avg**  | 0.580     | 0.520  | 0.490    | 219     |
| **Weighted Avg** | 0.740   | 0.820  | 0.760    | 219     |

#### LightGBM

| Class      | Precision | Recall | F1-Score | Support |
|------------|-----------|--------|----------|---------|
| 0          | 0.830     | 1.000  | 0.910    | 181     |
| 1          | 0.000     | 0.000  | 0.000    | 38      |
| **Accuracy**   |           |        | 0.830    | 219     |
| **Macro Avg**  | 0.410     | 0.500  | 0.450    | 219     |
| **Weighted Avg** | 0.680   | 0.830  | 0.750    | 219     |

#### Stacking Classifier

| Class      | Precision | Recall | F1-Score | Support |
|------------|-----------|--------|----------|---------|
| 0          | 0.840     | 0.970  | 0.900    | 181     |
| 1          | 0.440     | 0.110  | 0.170    | 38      |
| **Accuracy**   |           |        | 0.820    | 219     |
| **Macro Avg**  | 0.640     | 0.540  | 0.540    | 219     |
| **Weighted Avg** | 0.770   | 0.820  | 0.770    | 219     |

#### Voting Classifier

| Class      | Precision | Recall | F1-Score | Support |
|------------|-----------|--------|----------|---------|
| 0          | 0.830     | 0.990  | 0.900    | 181     |
| 1          | 0.000     | 0.000  | 0.000    | 38      |
| **Accuracy**   |           |        | 0.820    | 219     |
| **Macro Avg**  | 0.410     | 0.500  | 0.450    | 219     |
| **Weighted Avg** | 0.680   | 0.820  | 0.750    | 219     |

### ğŸ“Š Gradient Boosting Performance

![Gradient Boosting Performance Placeholder](outputs/gradient_boosting.jpg)

### ğŸ§® LightGBM Confusion Matrix

![LightGBM Confusion Matrix Placeholder](outputs/LightGBM.jpg)

### ğŸ¤– Stacking Classifier Results

![Stacking Classifier Results Placeholder](outputs/Stacking_Classifier.jpg)

### ğŸ—³ï¸ Voting Classifier Results

![Voting Classifier Results Placeholder](outputs/Voting_Classifier.jpg)

### ğŸ“ˆ Model Performance Comparison

![Model Performance Comparison Placeholder](outputs/Model_performance.jpg)

### ğŸ… ROC Curves for All Models

![ROC Curves Placeholder](outputs/ROC_curves.jpg)
```

